{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnCObXASRJ6V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 함수 정의: 특정 시간 단계에서 계수를 추출하고 브로드캐스팅을 위해 텐서 형태를 조정\n",
        "def extract(v, t, x_shape):\n",
        "    \"\"\"\n",
        "    Extract some coefficients at specified timesteps, then reshape to\n",
        "    [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
        "    \"\"\"\n",
        "    out = torch.gather(v, index=t, dim=0).float()  # t에 해당하는 인덱스에서 v값을 추출하고 float 타입으로 변환\n",
        "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))  # 추출된 값을 x_shape에 맞게 재배열\n",
        "\n",
        "# 가우시안 확산 트레이너 클래스 정의\n",
        "class GaussianDiffusionTrainer(nn.Module):\n",
        "    def __init__(self, model, beta_1, beta_T, T):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model  # 학습할 모델\n",
        "        self.T = T  # 확산 단계 수\n",
        "\n",
        "        # 베타 값(노이즈 제어 계수)을 베타_1에서 베타_T까지 선형으로 초기화\n",
        "        self.register_buffer('betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas  # 알파 값 계산\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)  # 알파 값의 누적 곱 계산\n",
        "\n",
        "        # 누적된 알파 값에 대한 제곱근 등록\n",
        "        self.register_buffer('sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
        "        self.register_buffer('sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
        "\n",
        "    def forward(self, x_0):\n",
        "        \"\"\"\n",
        "        Algorithm 1: 확산 프로세스를 시뮬레이션하고 손실을 계산합니다.\n",
        "        \"\"\"\n",
        "        t = torch.randint(self.T, size=(x_0.shape[0], ), device=x_0.device)  # 무작위 시간 단계 선택\n",
        "        noise = torch.randn_like(x_0)  # x_0과 같은 크기의 노이즈 생성\n",
        "        x_t = (\n",
        "            extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0 +  # 확산된 이미지 계산\n",
        "            extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise\n",
        "        )\n",
        "        loss = F.mse_loss(self.model(x_t, t), noise, reduction='none')  # 모델이 생성한 노이즈와 실제 노이즈의 MSE 손실\n",
        "        return loss  # 손실 반환\n",
        "\n",
        "# 가우시안 확산 샘플러 클래스 정의\n",
        "class GaussianDiffusionSampler(nn.Module):\n",
        "    # 초기화 함수\n",
        "    def __init__(self, model, beta_1, beta_T, T, img_size=32,\n",
        "                 mean_type='eps', var_type='fixedlarge'):\n",
        "        assert mean_type in ['xprev', 'xstart', 'epsilon']  # 평균 타입이 유효한지 확인\n",
        "        assert var_type in ['fixedlarge', 'fixedsmall']  # 분산 타입이 유효한지 확인\n",
        "        super().__init__()\n",
        "\n",
        "        # 클래스 변수 초기화\n",
        "        self.model = model  # 확산 모델\n",
        "        self.T = T  # 확산 과정의 총 단계 수\n",
        "        self.img_size = img_size  # 이미지 크기\n",
        "        self.mean_type = mean_type  # 평균 계산 방식\n",
        "        self.var_type = var_type  # 분산 계산 방식\n",
        "\n",
        "        # 베타 계수 초기화\n",
        "        self.register_buffer(\n",
        "            'betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas  # 알파 계수 계산\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)  # 알파의 누적곱\n",
        "        alphas_bar_prev = F.pad(alphas_bar, [1, 0], value=1)[:T]  # 알파 누적곱의 이전 값들에 대한 배열\n",
        "\n",
        "        # 확산 과정에 필요한 계수 등록\n",
        "        self.register_buffer(\n",
        "            'sqrt_recip_alphas_bar', torch.sqrt(1. / alphas_bar))  # 알파 누적곱의 역수의 제곱근\n",
        "        self.register_buffer(\n",
        "            'sqrt_recipm1_alphas_bar', torch.sqrt(1. / alphas_bar - 1))  # 역확산 계수\n",
        "\n",
        "        # 역확산 과정의 후방 분산 계산\n",
        "        self.register_buffer(\n",
        "            'posterior_var',\n",
        "            self.betas * (1. - alphas_bar_prev) / (1. - alphas_bar))\n",
        "        # 분산 계산 결과의 로그 값 클리핑 (분산이 0이 되는 경우 방지)\n",
        "        self.register_buffer(\n",
        "            'posterior_log_var_clipped',\n",
        "            torch.log(torch.cat([self.posterior_var[1:2], self.posterior_var[1:]])))\n",
        "        self.register_buffer(\n",
        "            'posterior_mean_coef1',\n",
        "            torch.sqrt(alphas_bar_prev) * self.betas / (1. - alphas_bar))  # 후방 평균 계수1\n",
        "        self.register_buffer(\n",
        "            'posterior_mean_coef2',\n",
        "            torch.sqrt(alphas) * (1. - alphas_bar_prev) / (1. - alphas_bar))  # 후방 평균 계수2\n",
        "\n",
        "    # 확산 후방의 평균과 분산 계산\n",
        "    def q_mean_variance(self, x_0, x_t, t):\n",
        "        posterior_mean = (\n",
        "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_0 +\n",
        "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
        "        )\n",
        "        posterior_log_var_clipped = extract(\n",
        "            self.posterior_log_var_clipped, t, x_t.shape)\n",
        "        return posterior_mean, posterior_log_var_clipped\n",
        "\n",
        "    # 엡실론을 사용하여 x_0 예측\n",
        "    def predict_xstart_from_eps(self, x_t, t, eps):\n",
        "        return (\n",
        "            extract(self.sqrt_recip_alphas_bar, t, x_t.shape) * x_t -\n",
        "            extract(self.sqrt_recipm1_alphas_bar, t, x_t.shape) * eps\n",
        "        )\n",
        "\n",
        "    # 이전 x를 사용하여 x_0 예측\n",
        "    def predict_xstart_from_xprev(self, x_t, t, xprev):\n",
        "        return (\n",
        "            extract(1. / self.posterior_mean_coef1, t, x_t.shape) * xprev -\n",
        "            extract(self.posterior_mean_coef2 / self.posterior_mean_coef1, t, x_t.shape) * x_t\n",
        "        )\n",
        "\n",
        "    # 평균과 분산 계산 및 반환\n",
        "    def p_mean_variance(self, x_t, t):\n",
        "        model_log_var = {\n",
        "            'fixedlarge': torch.log(torch.cat([self.posterior_var[1:2], self.betas[1:]])),\n",
        "            'fixedsmall': self.posterior_log_var_clipped,\n",
        "        }[self.var_type]\n",
        "        model_log_var = extract(model_log_var, t, x_t.shape)\n",
        "\n",
        "        if self.mean_type == 'xprev':\n",
        "            x_prev = self.model(x_t, t)\n",
        "            x_0 = self.predict_xstart_from_xprev(x_t, t, xprev=x_prev)\n",
        "            model_mean = x_prev\n",
        "        elif self.mean_type == 'xstart':\n",
        "            x_0 = self.model(x_t, t)\n",
        "            model_mean, _ = self.q_mean_variance(x_0, x_t, t)\n",
        "        elif self.mean_type == 'epsilon':\n",
        "            eps = self.model(x_t, t)\n",
        "            x_0 = self.predict_xstart_from_eps(x_t, t, eps=eps)\n",
        "            model_mean, _ = self.q_mean_variance(x_0, x_t, t)\n",
        "        else:\n",
        "            raise NotImplementedError(self.mean_type)\n",
        "        x_0 = torch.clip(x_0, -1., 1.)\n",
        "\n",
        "        return model_mean, model_log_var\n",
        "\n",
        "    # 전체 확산 과정 시뮬레이션 (역확산)\n",
        "    def forward(self, x_T):\n",
        "        x_t = x_T\n",
        "        for time_step in reversed(range(self.T)):  # T부터 0까지 역순으로 반복\n",
        "            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step\n",
        "            mean, log_var = self.p_mean_variance(x_t=x_t, t=t)\n",
        "            if time_step > 0:\n",
        "                noise = torch.randn_like(x_t)\n",
        "            else:\n",
        "                noise = 0\n",
        "            x_t = mean + torch.exp(0.5 * log_var) * noise\n",
        "        x_0 = x_t\n",
        "        return torch.clip(x_0, -1, 1)  # 최종 이미지 클리핑 및 반환"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, T, d_model, dim):\n",
        "        assert d_model % 2 == 0\n",
        "        super().__init__()\n",
        "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
        "        emb = torch.exp(-emb)\n",
        "        pos = torch.arange(T).float()\n",
        "        emb = pos[:, None] * emb[None, :]\n",
        "        assert list(emb.shape) == [T, d_model // 2]\n",
        "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "        assert list(emb.shape) == [T, d_model // 2, 2]\n",
        "        emb = emb.view(T, d_model)\n",
        "\n",
        "        self.timembedding = nn.Sequential(\n",
        "            nn.Embedding.from_pretrained(emb),\n",
        "            nn.Linear(d_model, dim),\n",
        "            Swish(),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                init.xavier_uniform_(module.weight)\n",
        "                init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, t):\n",
        "        emb = self.timembedding(t)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        init.xavier_uniform_(self.main.weight)\n",
        "        init.zeros_(self.main.bias)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        x = self.main(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        init.xavier_uniform_(self.main.weight)\n",
        "        init.zeros_(self.main.bias)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        _, _, H, W = x.shape\n",
        "        x = F.interpolate(\n",
        "            x, scale_factor=2, mode='nearest')\n",
        "        x = self.main(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
        "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        for module in [self.proj_q, self.proj_k, self.proj_v, self.proj]:\n",
        "            init.xavier_uniform_(module.weight)\n",
        "            init.zeros_(module.bias)\n",
        "        init.xavier_uniform_(self.proj.weight, gain=1e-5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.group_norm(x)\n",
        "        q = self.proj_q(h)\n",
        "        k = self.proj_k(h)\n",
        "        v = self.proj_v(h)\n",
        "\n",
        "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        k = k.view(B, C, H * W)\n",
        "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
        "        assert list(w.shape) == [B, H * W, H * W]\n",
        "        w = F.softmax(w, dim=-1)\n",
        "\n",
        "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        h = torch.bmm(w, v)\n",
        "        assert list(h.shape) == [B, H * W, C]\n",
        "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
        "        h = self.proj(h)\n",
        "\n",
        "        return x + h\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=False):\n",
        "        super().__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(32, in_ch),\n",
        "            Swish(),\n",
        "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        self.temb_proj = nn.Sequential(\n",
        "            Swish(),\n",
        "            nn.Linear(tdim, out_ch),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            Swish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        if in_ch != out_ch:\n",
        "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "        if attn:\n",
        "            self.attn = AttnBlock(out_ch)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                init.xavier_uniform_(module.weight)\n",
        "                init.zeros_(module.bias)\n",
        "        init.xavier_uniform_(self.block2[-1].weight, gain=1e-5)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = self.block1(x)\n",
        "        h += self.temb_proj(temb)[:, :, None, None]\n",
        "        h = self.block2(h)\n",
        "\n",
        "        h = h + self.shortcut(x)\n",
        "        h = self.attn(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, ch, ch_mult, attn, num_res_blocks, dropout):\n",
        "        super().__init__()\n",
        "        assert all([i < len(ch_mult) for i in attn]), 'attn index out of bound'\n",
        "        tdim = ch * 4\n",
        "        self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
        "\n",
        "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
        "        self.downblocks = nn.ModuleList()\n",
        "        chs = [ch]  # record output channel when dowmsample for upsample\n",
        "        now_ch = ch\n",
        "        for i, mult in enumerate(ch_mult):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks):\n",
        "                self.downblocks.append(ResBlock(\n",
        "                    in_ch=now_ch, out_ch=out_ch, tdim=tdim,\n",
        "                    dropout=dropout, attn=(i in attn)))\n",
        "                now_ch = out_ch\n",
        "                chs.append(now_ch)\n",
        "            if i != len(ch_mult) - 1:\n",
        "                self.downblocks.append(DownSample(now_ch))\n",
        "                chs.append(now_ch)\n",
        "\n",
        "        self.middleblocks = nn.ModuleList([\n",
        "            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
        "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
        "        ])\n",
        "\n",
        "        self.upblocks = nn.ModuleList()\n",
        "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks + 1):\n",
        "                self.upblocks.append(ResBlock(\n",
        "                    in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim,\n",
        "                    dropout=dropout, attn=(i in attn)))\n",
        "                now_ch = out_ch\n",
        "            if i != 0:\n",
        "                self.upblocks.append(UpSample(now_ch))\n",
        "        assert len(chs) == 0\n",
        "\n",
        "        self.tail = nn.Sequential(\n",
        "            nn.GroupNorm(32, now_ch),\n",
        "            Swish(),\n",
        "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
        "        )\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        init.xavier_uniform_(self.head.weight)\n",
        "        init.zeros_(self.head.bias)\n",
        "        init.xavier_uniform_(self.tail[-1].weight, gain=1e-5)\n",
        "        init.zeros_(self.tail[-1].bias)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Timestep embedding\n",
        "        temb = self.time_embedding(t)\n",
        "        # Downsampling\n",
        "        h = self.head(x)\n",
        "        hs = [h]\n",
        "        for layer in self.downblocks:\n",
        "            h = layer(h, temb)\n",
        "            hs.append(h)\n",
        "        # Middle\n",
        "        for layer in self.middleblocks:\n",
        "            h = layer(h, temb)\n",
        "        # Upsampling\n",
        "        for layer in self.upblocks:\n",
        "            if isinstance(layer, ResBlock):\n",
        "                h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = layer(h, temb)\n",
        "        h = self.tail(h)\n",
        "\n",
        "        assert len(hs) == 0\n",
        "        return h\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    batch_size = 8\n",
        "    model = UNet(\n",
        "        T=1000, ch=128, ch_mult=[1, 2, 2, 2], attn=[1],\n",
        "        num_res_blocks=2, dropout=0.1)\n",
        "    x = torch.randn(batch_size, 3, 32, 32)\n",
        "    t = torch.randint(1000, (batch_size, ))\n",
        "    y = model(x, t)"
      ],
      "metadata": {
        "id": "Ugj3jt-hZ-6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT50ZWuuHGc0",
        "outputId": "30336415-b1c4-4414-de41-f8c7cd3c1c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "from absl import app, flags\n",
        "\n",
        "import torch\n",
        "from tensorboardX import SummaryWriter\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torchvision import transforms\n",
        "from tqdm import trange"
      ],
      "metadata": {
        "id": "STE3M_ImJlTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# absl 플래그를 사용해 설정 관리\n",
        "FLAGS = flags.FLAGS\n",
        "flags.DEFINE_bool('train', False, help='train from scratch')\n",
        "flags.DEFINE_bool('eval', False, help='load ckpt.pt and evaluate FID and IS')\n",
        "# UNet 설정 관련 플래그\n",
        "flags.DEFINE_integer('ch', 128, help='base channel of UNet')  # UNet의 기본 채널 수\n",
        "flags.DEFINE_multi_integer('ch_mult', [1, 2, 2, 2], help='channel multiplier')  # 채널 배수 설정\n",
        "flags.DEFINE_multi_integer('attn', [1], help='add attention to these levels')  # 어텐션 추가 레벨\n",
        "flags.DEFINE_integer('num_res_blocks', 2, help='# resblock in each level')  # 각 레벨의 레지듀얼 블록 수\n",
        "flags.DEFINE_float('dropout', 0.1, help='dropout rate of resblock')  # 레지듀얼 블록의 드롭아웃 비율\n",
        "# 가우시안 확산 설정 관련 플래그\n",
        "flags.DEFINE_float('beta_1', 1e-4, help='start beta value')  # 확산 시작 베타 값\n",
        "flags.DEFINE_float('beta_T', 0.02, help='end beta value')  # 확산 종료 베타 값\n",
        "flags.DEFINE_integer('T', 1000, help='total diffusion steps')  # 확산 단계의 총 수\n",
        "flags.DEFINE_enum('mean_type', 'epsilon', ['xprev', 'xstart', 'epsilon'], help='predict variable')  # 예측 변수 타입\n",
        "flags.DEFINE_enum('var_type', 'fixedlarge', ['fixedlarge', 'fixedsmall'], help='variance type')  # 분산 타입\n",
        "# 훈련 설정 관련 플래그\n",
        "flags.DEFINE_float('lr', 2e-4, help='target learning rate')  # 학습률\n",
        "flags.DEFINE_float('grad_clip', 1., help=\"gradient norm clipping\")  # 그래디언트 클리핑\n",
        "flags.DEFINE_integer('total_steps', 800000, help='total training steps')  # 총 훈련 스텝 수\n",
        "flags.DEFINE_integer('img_size', 32, help='image size')  # 이미지 크기\n",
        "flags.DEFINE_integer('warmup', 5000, help='learning rate warmup')  # 학습률 웜업 스텝 수\n",
        "flags.DEFINE_integer('batch_size', 128, help='batch size')  # 배치 크기\n",
        "flags.DEFINE_integer('num_workers', 4, help='workers of Dataloader')  # 데이터 로더의 워커 수\n",
        "flags.DEFINE_float('ema_decay', 0.9999, help=\"ema decay rate\")  # EMA 감쇠율\n",
        "flags.DEFINE_bool('parallel', False, help='multi gpu training')  # 다중 GPU 훈련 사용 여부\n",
        "# 로깅 및 샘플링 설정 관련 플래그\n",
        "flags.DEFINE_string('logdir', '/content/drive/Shareddrives/Project/real_vs_fake', 'log directory') # 로그 디렉토리 경로\n",
        "flags.DEFINE_integer('sample_size', 64, \"sampling size of images\")  # 샘플링 이미지 수\n",
        "flags.DEFINE_integer('sample_step', 1000, help='frequency of sampling')  # 샘플링 빈도\n",
        "# 평가 설정 관련 플래그\n",
        "flags.DEFINE_integer('save_step', 5000, help='frequency of saving checkpoints, 0 to disable during training')  # 체크포인트 저장 빈도\n",
        "flags.DEFINE_integer('eval_step', 0, help='frequency of evaluating model, 0 to disable during training')  # 모델 평가 빈도\n",
        "flags.DEFINE_integer('num_images', 50000, help='the number of generated images for evaluation')  # 평가를 위해 생성할 이미지 수\n",
        "flags.DEFINE_bool('fid_use_torch', False, help='calculate IS and FID on gpu')  # GPU에서 FID와 IS 계산 여부\n",
        "flags.DEFINE_string('fid_cache', '/content/drive/Shareddrives/Project/real_vs_fake', 'FID cache')   # FID 캐시 파일 경로\n",
        "\n",
        "device = torch.device('cuda:0')  # GPU 디바이스 설정"
      ],
      "metadata": {
        "id": "a8mOWzhiaS_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "76222581-ab0e-4f0c-9095-6a9ccb2fe737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DuplicateFlagError",
          "evalue": "The flag 'ch' is defined twice. First from /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py, Second from /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py.  Description from first occurrence: base channel of UNet",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6fd32a372c00>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#flags.DEFINE_bool('eval', False, help='load ckpt.pt and evaluate FID and IS')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# UNet 설정 관련 플래그\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'base channel of UNet'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# UNet의 기본 채널 수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_multi_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ch_mult'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channel multiplier'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 채널 배수 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_multi_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'add attention to these levels'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 어텐션 추가 레벨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[0;34m(name, default, help, lower_bound, upper_bound, flag_values, required, **args)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegerParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m   result = DEFINE(\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, required, **args)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdefined\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \"\"\"\n\u001b[0;32m--> 100\u001b[0;31m   return DEFINE_flag(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0m_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       module_name, required)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name, required)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'ch' is defined twice. First from /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py, Second from /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py.  Description from first occurrence: base channel of UNet"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "try:\n",
        "    from torchvision.models.utils import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "# Inception weights ported to Pytorch from\n",
        "# http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n",
        "FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'\n",
        "\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,      # First max pooling features\n",
        "        192: 1,     # Second max pooling featurs\n",
        "        768: 2,     # Pre-aux classifier features\n",
        "        2048: 3,    # Final average pooling features\n",
        "        'prob': 4,  # softmax layer\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False,\n",
        "                 use_fid_inception=True):\n",
        "        \"\"\"Build pretrained InceptionV3\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        output_blocks : list of int\n",
        "            Indices of blocks to return features of. Possible values are:\n",
        "                - 0: corresponds to output of first max pooling\n",
        "                - 1: corresponds to output of second max pooling\n",
        "                - 2: corresponds to output which is fed to aux classifier\n",
        "                - 3: corresponds to output of final average pooling\n",
        "        resize_input : bool\n",
        "            If true, bilinearly resizes input to width and height 299 before\n",
        "            feeding input to model. As the network without fully connected\n",
        "            layers is fully convolutional, it should be able to handle inputs\n",
        "            of arbitrary size, so resizing might not be strictly needed\n",
        "        normalize_input : bool\n",
        "            If true, scales the input from range (0, 1) to the range the\n",
        "            pretrained Inception network expects, namely (-1, 1)\n",
        "        requires_grad : bool\n",
        "            If true, parameters of the model require gradients. Possibly useful\n",
        "            for finetuning the network\n",
        "        use_fid_inception : bool\n",
        "            If true, uses the pretrained Inception model used in Tensorflow's\n",
        "            FID implementation. If false, uses the pretrained Inception model\n",
        "            available in torchvision. The FID Inception model has different\n",
        "            weights and a slightly different structure from torchvision's\n",
        "            Inception model. If you want to compute FID scores, you are\n",
        "            strongly advised to set this parameter to true to get comparable\n",
        "            results.\n",
        "        \"\"\"\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        # assert self.last_needed_block <= 3, \\\n",
        "        #     'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        if use_fid_inception:\n",
        "            inception = fid_inception_v3()\n",
        "        else:\n",
        "            inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        if self.last_needed_block >= 4:\n",
        "            self.fc = inception.fc\n",
        "            self.fc.bias = None\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        if self.last_needed_block >= 4:\n",
        "            x = F.dropout(x, training=self.training)\n",
        "            # N x 2048 x 1 x 1\n",
        "            x = torch.flatten(x, 1)\n",
        "            # N x 2048\n",
        "            x = self.fc(x)\n",
        "            x = F.softmax(x, dim=1)\n",
        "            outp.append(x)\n",
        "\n",
        "        return outp\n",
        "\n",
        "\n",
        "def fid_inception_v3():\n",
        "    \"\"\"Build pretrained Inception model for FID computation\n",
        "\n",
        "    The Inception model for FID computation uses a different set of weights\n",
        "    and has a slightly different structure than torchvision's Inception.\n",
        "\n",
        "    This method first constructs torchvision's Inception and then patches the\n",
        "    necessary parts that are different in the FID Inception model.\n",
        "    \"\"\"\n",
        "    inception = models.inception_v3(num_classes=1008,\n",
        "                                    aux_logits=False,\n",
        "                                    pretrained=False)\n",
        "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
        "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
        "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
        "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
        "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
        "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
        "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
        "\n",
        "    state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n",
        "    inception.load_state_dict(state_dict)\n",
        "    return inception\n",
        "\n",
        "\n",
        "class FIDInceptionA(models.inception.InceptionA):\n",
        "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels, pool_features):\n",
        "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionC(models.inception.InceptionC):\n",
        "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels, channels_7x7):\n",
        "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_1(models.inception.InceptionE):\n",
        "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_2(models.inception.InceptionE):\n",
        "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: The FID Inception model uses max pooling instead of average\n",
        "        # pooling. This is likely an error in this specific Inception\n",
        "        # implementation, as other Inception models use average pooling here\n",
        "        # (which matches the description in the paper).\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)"
      ],
      "metadata": {
        "id": "URg4Qi84K_2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from scipy import linalg\n",
        "from tqdm import tqdm\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "\n",
        "DIM = 2048\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "\n",
        "def torch_cov(m, rowvar=False):\n",
        "    '''Estimate a covariance matrix given data.\n",
        "    Covariance indicates the level to which two variables vary together.\n",
        "    If we examine N-dimensional samples, `X = [x_1, x_2, ... x_N]^T`,\n",
        "    then the covariance matrix element `C_{ij}` is the covariance of\n",
        "    `x_i` and `x_j`. The element `C_{ii}` is the variance of `x_i`.\n",
        "    Args:\n",
        "        m: A 1-D or 2-D array containing multiple variables and observations.\n",
        "            Each row of `m` represents a variable, and each column a single\n",
        "            observation of all those variables.\n",
        "        rowvar: If `rowvar` is True, then each row represents a\n",
        "            variable, with observations in the columns. Otherwise, the\n",
        "            relationship is transposed: each column represents a variable,\n",
        "            while the rows contain observations.\n",
        "    Returns:\n",
        "        The covariance matrix of the variables.\n",
        "    '''\n",
        "    if m.dim() > 2:\n",
        "        raise ValueError('m has more than 2 dimensions')\n",
        "    if m.dim() < 2:\n",
        "        m = m.view(1, -1)\n",
        "    if not rowvar and m.size(0) != 1:\n",
        "        m = m.t()\n",
        "    # m = m.type(torch.double)  # uncomment this line if desired\n",
        "    fact = 1.0 / (m.size(1) - 1)\n",
        "    m -= torch.mean(m, dim=1, keepdim=True)\n",
        "    mt = m.t()  # if complex: mt = m.t().conj()\n",
        "    return fact * m.matmul(mt).squeeze()\n",
        "\n",
        "\n",
        "# Pytorch implementation of matrix sqrt, from Tsung-Yu Lin, and Subhransu Maji\n",
        "# https://github.com/msubhransu/matrix-sqrt\n",
        "def sqrt_newton_schulz(A, numIters, dtype=None):\n",
        "    with torch.no_grad():\n",
        "        if dtype is None:\n",
        "            dtype = A.type()\n",
        "        batchSize = A.shape[0]\n",
        "        dim = A.shape[1]\n",
        "        normA = A.mul(A).sum(dim=1).sum(dim=1).sqrt()\n",
        "        Y = A.div(normA.view(batchSize, 1, 1).expand_as(A))\n",
        "        K = torch.eye(dim, dim).view(1, dim, dim).repeat(batchSize, 1, 1)\n",
        "        Z = torch.eye(dim, dim).view(1, dim, dim).repeat(batchSize, 1, 1)\n",
        "        K = K.type(dtype)\n",
        "        Z = Z.type(dtype)\n",
        "        for i in range(numIters):\n",
        "            T = 0.5 * (3.0 * K - Z.bmm(Y))\n",
        "            Y = Y.bmm(T)\n",
        "            Z = T.bmm(Z)\n",
        "        sA = Y*torch.sqrt(normA).view(batchSize, 1, 1).expand_as(A)\n",
        "    return sA\n",
        "\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6,\n",
        "                               use_torch=False):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "\n",
        "    Stable version by Dougal J. Sutherland.\n",
        "\n",
        "    Params:\n",
        "    -- mu1   : Numpy array containing the activations of a layer of the\n",
        "               inception net (like returned by the function 'get_predictions')\n",
        "               for generated samples.\n",
        "    -- mu2   : The sample mean over activations, precalculated on an\n",
        "               representative data set.\n",
        "    -- sigma1: The covariance matrix over activations for generated samples.\n",
        "    -- sigma2: The covariance matrix over activations, precalculated on an\n",
        "               representative data set.\n",
        "\n",
        "    Returns:\n",
        "    --   : The Frechet Distance.\n",
        "    \"\"\"\n",
        "\n",
        "    if use_torch:\n",
        "        assert mu1.shape == mu2.shape, \\\n",
        "            'Training and test mean vectors have different lengths'\n",
        "        assert sigma1.shape == sigma2.shape, \\\n",
        "            'Training and test covariances have different dimensions'\n",
        "\n",
        "        diff = mu1 - mu2\n",
        "        # Run 50 itrs of newton-schulz to get the matrix sqrt of\n",
        "        # sigma1 dot sigma2\n",
        "        covmean = sqrt_newton_schulz(sigma1.mm(sigma2).unsqueeze(0), 50)\n",
        "        if torch.any(torch.isnan(covmean)):\n",
        "            return float('nan')\n",
        "        covmean = covmean.squeeze()\n",
        "        out = (diff.dot(diff) +\n",
        "               torch.trace(sigma1) +\n",
        "               torch.trace(sigma2) -\n",
        "               2 * torch.trace(covmean)).cpu().item()\n",
        "    else:\n",
        "        mu1 = np.atleast_1d(mu1)\n",
        "        mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "        sigma1 = np.atleast_2d(sigma1)\n",
        "        sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "        assert mu1.shape == mu2.shape, \\\n",
        "            'Training and test mean vectors have different lengths'\n",
        "        assert sigma1.shape == sigma2.shape, \\\n",
        "            'Training and test covariances have different dimensions'\n",
        "\n",
        "        diff = mu1 - mu2\n",
        "\n",
        "        # Product might be almost singular\n",
        "        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "        if not np.isfinite(covmean).all():\n",
        "            msg = ('fid calculation produces singular product; '\n",
        "                   'adding %s to diagonal of cov estimates') % eps\n",
        "            print(msg)\n",
        "            offset = np.eye(sigma1.shape[0]) * eps\n",
        "            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "        # Numerical error might give slight imaginary component\n",
        "        if np.iscomplexobj(covmean):\n",
        "            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "                m = np.max(np.abs(covmean.imag))\n",
        "                raise ValueError('Imaginary component {}'.format(m))\n",
        "            covmean = covmean.real\n",
        "\n",
        "        tr_covmean = np.trace(covmean)\n",
        "\n",
        "        out = (diff.dot(diff) +\n",
        "               np.trace(sigma1) +\n",
        "               np.trace(sigma2) -\n",
        "               2 * tr_covmean)\n",
        "    return out\n",
        "\n",
        "\n",
        "def get_statistics(images, num_images=None, batch_size=50, use_torch=False,\n",
        "                   verbose=False, parallel=False):\n",
        "    \"\"\"when `images` is a python generator, `num_images` should be given\"\"\"\n",
        "\n",
        "    if num_images is None:\n",
        "        try:\n",
        "            num_images = len(images)\n",
        "        except:\n",
        "            raise ValueError(\n",
        "                \"when `images` is not a list like object (e.g. generator), \"\n",
        "                \"`num_images` should be given\")\n",
        "\n",
        "    block_idx1 = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "    model = InceptionV3([block_idx1]).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if parallel:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    if use_torch:\n",
        "        fid_acts = torch.empty((num_images, 2048)).to(device)\n",
        "    else:\n",
        "        fid_acts = np.empty((num_images, 2048))\n",
        "\n",
        "    iterator = iter(tqdm(\n",
        "        images, total=num_images,\n",
        "        dynamic_ncols=True, leave=False, disable=not verbose,\n",
        "        desc=\"get_inception_and_fid_score\"))\n",
        "\n",
        "    start = 0\n",
        "    while True:\n",
        "        batch_images = []\n",
        "        # get a batch of images from iterator\n",
        "        try:\n",
        "            for _ in range(batch_size):\n",
        "                batch_images.append(next(iterator))\n",
        "        except StopIteration:\n",
        "            if len(batch_images) == 0:\n",
        "                break\n",
        "            pass\n",
        "        batch_images = np.stack(batch_images, axis=0)\n",
        "        end = start + len(batch_images)\n",
        "\n",
        "        # calculate inception feature\n",
        "        batch_images = torch.from_numpy(batch_images).type(torch.FloatTensor)\n",
        "        batch_images = batch_images.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch_images)\n",
        "            if use_torch:\n",
        "                fid_acts[start: end] = pred[0].view(-1, 2048)\n",
        "            else:\n",
        "                fid_acts[start: end] = pred[0].view(-1, 2048).cpu().numpy()\n",
        "        start = end\n",
        "\n",
        "    if use_torch:\n",
        "        m1 = torch.mean(fid_acts, axis=0)\n",
        "        s1 = torch_cov(fid_acts, rowvar=False)\n",
        "    else:\n",
        "        m1 = np.mean(fid_acts, axis=0)\n",
        "        s1 = np.cov(fid_acts, rowvar=False)\n",
        "    return m1, s1\n",
        "\n",
        "\n",
        "def get_fid_score(stats_cache, images, num_images=None, batch_size=50,\n",
        "                  use_torch=False, verbose=False, parallel=False):\n",
        "    m1, s1 = get_statistics(\n",
        "        images, num_images, batch_size, use_torch, verbose, parallel)\n",
        "\n",
        "    f = np.load(stats_cache)\n",
        "    m2, s2 = f['mu'][:], f['sigma'][:]\n",
        "    f.close()\n",
        "    if use_torch:\n",
        "        m2 = torch.tensor(m2).to(m1.dtype)\n",
        "        s2 = torch.tensor(s2).to(s1.dtype)\n",
        "    fid_value = calculate_frechet_distance(m1, s1, m2, s2, use_torch=use_torch)\n",
        "\n",
        "    if use_torch:\n",
        "        fid_value = fid_value.cpu().item()\n",
        "    return fid_value"
      ],
      "metadata": {
        "id": "MjqAqxlAK9iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import types\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "\n",
        "def get_inception_and_fid_score(images, fid_cache, num_images=None,\n",
        "                                splits=10, batch_size=50,\n",
        "                                use_torch=False,\n",
        "                                verbose=False,\n",
        "                                parallel=False):\n",
        "    \"\"\"when `images` is a python generator, `num_images` should be given\"\"\"\n",
        "\n",
        "    if num_images is None and isinstance(images, types.GeneratorType):\n",
        "        raise ValueError(\n",
        "            \"when `images` is a python generator, \"\n",
        "            \"`num_images` should be given\")\n",
        "\n",
        "    if num_images is None:\n",
        "        num_images = len(images)\n",
        "\n",
        "    block_idx1 = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "    block_idx2 = InceptionV3.BLOCK_INDEX_BY_DIM['prob']\n",
        "    model = InceptionV3([block_idx1, block_idx2]).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if parallel:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    if use_torch:\n",
        "        fid_acts = torch.empty((num_images, 2048)).to(device)\n",
        "        is_probs = torch.empty((num_images, 1008)).to(device)\n",
        "    else:\n",
        "        fid_acts = np.empty((num_images, 2048))\n",
        "        is_probs = np.empty((num_images, 1008))\n",
        "\n",
        "    iterator = iter(tqdm(\n",
        "        images, total=num_images,\n",
        "        dynamic_ncols=True, leave=False, disable=not verbose,\n",
        "        desc=\"get_inception_and_fid_score\"))\n",
        "\n",
        "    start = 0\n",
        "    while True:\n",
        "        batch_images = []\n",
        "        # get a batch of images from iterator\n",
        "        try:\n",
        "            for _ in range(batch_size):\n",
        "                batch_images.append(next(iterator))\n",
        "        except StopIteration:\n",
        "            if len(batch_images) == 0:\n",
        "                break\n",
        "            pass\n",
        "        batch_images = np.stack(batch_images, axis=0)\n",
        "        end = start + len(batch_images)\n",
        "\n",
        "        # calculate inception feature\n",
        "        batch_images = torch.from_numpy(batch_images).type(torch.FloatTensor)\n",
        "        batch_images = batch_images.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch_images)\n",
        "            if use_torch:\n",
        "                fid_acts[start: end] = pred[0].view(-1, 2048)\n",
        "                is_probs[start: end] = pred[1]\n",
        "            else:\n",
        "                fid_acts[start: end] = pred[0].view(-1, 2048).cpu().numpy()\n",
        "                is_probs[start: end] = pred[1].cpu().numpy()\n",
        "        start = end\n",
        "\n",
        "    # Inception Score\n",
        "    scores = []\n",
        "    for i in range(splits):\n",
        "        part = is_probs[\n",
        "            (i * is_probs.shape[0] // splits):\n",
        "            ((i + 1) * is_probs.shape[0] // splits), :]\n",
        "        if use_torch:\n",
        "            kl = part * (\n",
        "                torch.log(part) -\n",
        "                torch.log(torch.unsqueeze(torch.mean(part, 0), 0)))\n",
        "            kl = torch.mean(torch.sum(kl, 1))\n",
        "            scores.append(torch.exp(kl))\n",
        "        else:\n",
        "            kl = part * (\n",
        "                np.log(part) -\n",
        "                np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
        "            kl = np.mean(np.sum(kl, 1))\n",
        "            scores.append(np.exp(kl))\n",
        "    if use_torch:\n",
        "        scores = torch.stack(scores)\n",
        "        is_score = (torch.mean(scores).cpu().item(),\n",
        "                    torch.std(scores).cpu().item())\n",
        "    else:\n",
        "        is_score = (np.mean(scores), np.std(scores))\n",
        "\n",
        "    # FID Score\n",
        "    f = np.load(fid_cache)\n",
        "    m2, s2 = f['mu'][:], f['sigma'][:]\n",
        "    f.close()\n",
        "    if use_torch:\n",
        "        m1 = torch.mean(fid_acts, axis=0)\n",
        "        s1 = torch_cov(fid_acts, rowvar=False)\n",
        "        m2 = torch.tensor(m2).to(m1.dtype).to(device)\n",
        "        s2 = torch.tensor(s2).to(s1.dtype).to(device)\n",
        "    else:\n",
        "        m1 = np.mean(fid_acts, axis=0)\n",
        "        s1 = np.cov(fid_acts, rowvar=False)\n",
        "    fid_score = calculate_frechet_distance(m1, s1, m2, s2, use_torch=use_torch)\n",
        "\n",
        "    del fid_acts, is_probs, scores, model\n",
        "    return is_score, fid_score"
      ],
      "metadata": {
        "id": "NAh1VaZkK1Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YA5ChQrxc516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773466e7-3fad-4345-d65c-f0b89d1ce2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def ema(source, target, decay):\n",
        "    source_dict = source.state_dict()\n",
        "    target_dict = target.state_dict()\n",
        "    for key in source_dict.keys():\n",
        "        target_dict[key].data.copy_(\n",
        "            target_dict[key].data * decay +\n",
        "            source_dict[key].data * (1 - decay))\n",
        "\n",
        "\n",
        "def infiniteloop(dataloader):\n",
        "    while True:\n",
        "        for x, y in iter(dataloader):\n",
        "            yield x\n",
        "\n",
        "\n",
        "def warmup_lr(step):\n",
        "    return min(step, FLAGS.warmup) / FLAGS.warmup\n",
        "\n",
        "\n",
        "def evaluate(sampler, model):\n",
        "    model.eval()\n",
        "    eval_transform = transforms.Compose([\n",
        "        transforms.Resize((FLAGS.img_size, FLAGS.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    eval_dataset = ImageFolder(root=dataset_path, transform=eval_transform)\n",
        "    eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=FLAGS.batch_size, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images = []\n",
        "        desc = \"generating images\"\n",
        "        for i in trange(0, FLAGS.num_images, FLAGS.batch_size, desc=desc):\n",
        "            batch_size = min(FLAGS.batch_size, FLAGS.num_images - i)\n",
        "            x_T = torch.randn((batch_size, 3, FLAGS.img_size, FLAGS.img_size))\n",
        "            batch_images = sampler(x_T.to(device)).cpu()\n",
        "            images.append((batch_images + 1) / 2)\n",
        "        images = torch.cat(images, dim=0).numpy()\n",
        "    model.train()\n",
        "    (IS, IS_std), FID = get_inception_and_fid_score(\n",
        "        images, FLAGS.fid_cache, num_images=FLAGS.num_images,\n",
        "        use_torch=FLAGS.fid_use_torch, verbose=True)\n",
        "    return (IS, IS_std), FID, images\n",
        "\n",
        "def train():\n",
        "    # 데이터셋에 적용할 전처리 정의\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Resize((FLAGS.img_size, FLAGS.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "    train_dataset = ImageFolder(root=FLAGS.validation_data, transform=train_transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=FLAGS.batch_size, shuffle=True, num_workers=FLAGS.num_workers)\n",
        "    datalooper = infiniteloop(train_loader)\n",
        "\n",
        "    # 모델 및 학습 관련 코드는 동일하게 유지\n",
        "    net_model = UNet(\n",
        "        T=FLAGS.T, ch=FLAGS.ch, ch_mult=FLAGS.ch_mult, attn=FLAGS.attn,\n",
        "        num_res_blocks=FLAGS.num_res_blocks, dropout=FLAGS.dropout)\n",
        "    ema_model = copy.deepcopy(net_model)\n",
        "    optim = torch.optim.Adam(net_model.parameters(), lr=FLAGS.lr)\n",
        "    sched = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=warmup_lr)\n",
        "\n",
        "    trainer = GaussianDiffusionTrainer(\n",
        "        net_model, FLAGS.beta_1, FLAGS.beta_T, FLAGS.T).to(device)\n",
        "    net_sampler = GaussianDiffusionSampler(\n",
        "        net_model, FLAGS.beta_1, FLAGS.beta_T, FLAGS.T, FLAGS.img_size,\n",
        "        FLAGS.mean_type, FLAGS.var_type).to(device)\n",
        "    ema_sampler = GaussianDiffusionSampler(\n",
        "        ema_model, FLAGS.beta_1, FLAGS.beta_T, FLAGS.T, FLAGS.img_size,\n",
        "        FLAGS.mean_type, FLAGS.var_type).to(device)\n",
        "    if FLAGS.parallel:\n",
        "        trainer = torch.nn.DataParallel(trainer)\n",
        "        net_sampler = torch.nn.DataParallel(net_sampler)\n",
        "        ema_sampler = torch.nn.DataParallel(ema_sampler)\n",
        "\n",
        "    # log setup\n",
        "    os.makedirs(os.path.join(FLAGS.logdir, 'sample'))\n",
        "    x_T = torch.randn(FLAGS.sample_size, 3, FLAGS.img_size, FLAGS.img_size)\n",
        "    x_T = x_T.to(device)\n",
        "    grid = (make_grid(next(iter(train_loader))[0][:FLAGS.sample_size]) + 1) / 2\n",
        "    writer = SummaryWriter(FLAGS.logdir)\n",
        "    writer.add_image('real_sample', grid)\n",
        "    writer.flush()\n",
        "    # backup all arguments\n",
        "    with open(os.path.join(FLAGS.logdir, \"flagfile.txt\"), 'w') as f:\n",
        "        f.write(FLAGS.flags_into_string())\n",
        "    # show model size\n",
        "    model_size = 0\n",
        "    for param in net_model.parameters():\n",
        "        model_size += param.data.nelement()\n",
        "    print('Model params: %.2f M' % (model_size / 1024 / 1024))\n",
        "\n",
        "    # start training\n",
        "    with trange(FLAGS.total_steps, dynamic_ncols=True) as pbar:\n",
        "        for step in pbar:\n",
        "            # train\n",
        "            optim.zero_grad()\n",
        "            x_0 = next(datalooper).to(device)\n",
        "            loss = trainer(x_0).mean()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                net_model.parameters(), FLAGS.grad_clip)\n",
        "            optim.step()\n",
        "            sched.step()\n",
        "            ema(net_model, ema_model, FLAGS.ema_decay)\n",
        "\n",
        "            # log\n",
        "            writer.add_scalar('loss', loss, step)\n",
        "            pbar.set_postfix(loss='%.3f' % loss)\n",
        "\n",
        "            # sample\n",
        "            if FLAGS.sample_step > 0 and step % FLAGS.sample_step == 0:\n",
        "                net_model.eval()\n",
        "                with torch.no_grad():\n",
        "                    x_0 = ema_sampler(x_T)\n",
        "                    grid = (make_grid(x_0) + 1) / 2\n",
        "                    path = os.path.join(\n",
        "                        FLAGS.logdir, 'sample', '%d.png' % step)\n",
        "                    save_image(grid, path)\n",
        "                    writer.add_image('sample', grid, step)\n",
        "                net_model.train()\n",
        "\n",
        "            # save\n",
        "            if FLAGS.save_step > 0 and step % FLAGS.save_step == 0:\n",
        "                ckpt = {\n",
        "                    'net_model': net_model.state_dict(),\n",
        "                    'ema_model': ema_model.state_dict(),\n",
        "                    'sched': sched.state_dict(),\n",
        "                    'optim': optim.state_dict(),\n",
        "                    'step': step,\n",
        "                    'x_T': x_T,\n",
        "                }\n",
        "                torch.save(ckpt, os.path.join(FLAGS.logdir, 'ckpt.pt'))\n",
        "\n",
        "            # evaluate\n",
        "            if FLAGS.eval_step > 0 and step % FLAGS.eval_step == 0:\n",
        "                net_IS, net_FID, _ = evaluate(net_sampler, net_model)\n",
        "                ema_IS, ema_FID, _ = evaluate(ema_sampler, ema_model)\n",
        "                metrics = {\n",
        "                    'IS': net_IS[0],\n",
        "                    'IS_std': net_IS[1],\n",
        "                    'FID': net_FID,\n",
        "                    'IS_EMA': ema_IS[0],\n",
        "                    'IS_std_EMA': ema_IS[1],\n",
        "                    'FID_EMA': ema_FID\n",
        "                }\n",
        "                pbar.write(\n",
        "                    \"%d/%d \" % (step, FLAGS.total_steps) +\n",
        "                    \", \".join('%s:%.3f' % (k, v) for k, v in metrics.items()))\n",
        "                for name, value in metrics.items():\n",
        "                    writer.add_scalar(name, value, step)\n",
        "                writer.flush()\n",
        "                with open(os.path.join(FLAGS.logdir, 'eval.txt'), 'a') as f:\n",
        "                    metrics['step'] = step\n",
        "                    f.write(json.dumps(metrics) + \"\\n\")\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "def eval():\n",
        "    # model setup\n",
        "    model = UNet(\n",
        "        T=FLAGS.T, ch=FLAGS.ch, ch_mult=FLAGS.ch_mult, attn=FLAGS.attn,\n",
        "        num_res_blocks=FLAGS.num_res_blocks, dropout=FLAGS.dropout)\n",
        "    sampler = GaussianDiffusionSampler(\n",
        "        model, FLAGS.beta_1, FLAGS.beta_T, FLAGS.T, img_size=FLAGS.img_size,\n",
        "        mean_type=FLAGS.mean_type, var_type=FLAGS.var_type).to(device)\n",
        "    if FLAGS.parallel:\n",
        "        sampler = torch.nn.DataParallel(sampler)\n",
        "\n",
        "    # load model and evaluate\n",
        "    ckpt = torch.load(os.path.join(FLAGS.logdir, 'ckpt.pt'))\n",
        "    model.load_state_dict(ckpt['net_model'])\n",
        "    (IS, IS_std), FID, samples = evaluate(sampler, model, FLAGS.evaluation_data)\n",
        "    print(\"Model     : IS:%6.3f(%.3f), FID:%7.3f\" % (IS, IS_std, FID))\n",
        "    save_image(\n",
        "        torch.tensor(samples[:256]),\n",
        "        os.path.join(FLAGS.logdir, 'samples.png'),\n",
        "        nrow=16)\n",
        "\n",
        "    model.load_state_dict(ckpt['ema_model'])\n",
        "    (IS, IS_std), FID, samples = evaluate(sampler, model, FLAGS.evaluation_data)\n",
        "    print(\"Model(EMA): IS:%6.3f(%.3f), FID:%7.3f\" % (IS, IS_std, FID))\n",
        "    save_image(\n",
        "        torch.tensor(samples[:256]),\n",
        "        os.path.join(FLAGS.logdir, 'samples_ema.png'),\n",
        "        nrow=16)\n",
        "\n",
        "def main(argv):\n",
        "    # suppress annoying inception_v3 initialization warning\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "    if FLAGS.train:\n",
        "        train()\n",
        "    if FLAGS.eval:\n",
        "        eval()\n",
        "    if not FLAGS.train and not FLAGS.eval:\n",
        "        print('Add --train and/or --eval to execute corresponding tasks')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "dOHxuJTeEO2Z",
        "outputId": "88aebaab-be2a-495f-d63b-2fa391c413d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnparsedFlagAccessError",
          "evalue": "Trying to access flag --train before flags were parsed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnparsedFlagAccessError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-59caa3c265bd>\u001b[0m in \u001b[0;36m<cell line: 205>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-59caa3c265bd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# suppress annoying inception_v3 initialization warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    479\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m       raise _exceptions.UnparsedFlagAccessError(\n\u001b[0m\u001b[1;32m    482\u001b[0m           'Trying to access flag --%s before flags were parsed.' % name)\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnparsedFlagAccessError\u001b[0m: Trying to access flag --train before flags were parsed."
          ]
        }
      ]
    }
  ]
}