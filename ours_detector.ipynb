{"cells":[{"cell_type":"markdown","source":["# Upload pre-trained from timm"],"metadata":{"id":"adLDPo9KVxLG"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hQ6aV8lRzNq","outputId":"176762ad-3f76-4844-efdf-ad16d60646b8","executionInfo":{"status":"ok","timestamp":1718959814435,"user_tz":-540,"elapsed":89744,"user":{"displayName":"22 ML","userId":"00831946089676507549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting timm\n","  Downloading timm-1.0.7-py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.4)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch->timm)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 timm-1.0.7\n"]}],"source":["pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178,"referenced_widgets":["4f8a484727bf41d88f619c4fd2b9a24e","02149e2905b041af8e110243186e6a38","98e0b5be2ac24819a12ee22e3264c3ef","6e2fa310759342bebd04d82325cd248a","06b56f7d430f44fcb1057e9f02ffa588","2a5393c976c04740b45966874927933b","4d021402675e41cfa8edc31b7cec3c5e","02e092874ae349faa22a44593e3c9654","af67225a89bc4479a4bb9794bfa6c1cc","c449fe02d6c64ecb8f08be0e0bffec37","a200054d3d81493da8a6542b4ff50ea8"]},"id":"hZF2OZzfSdNo","executionInfo":{"status":"ok","timestamp":1718959840072,"user_tz":-540,"elapsed":25639,"user":{"displayName":"22 ML","userId":"00831946089676507549"}},"outputId":"9a369653-7b54-4de0-dd9b-d7623814557d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f8a484727bf41d88f619c4fd2b9a24e"}},"metadata":{}}],"source":["import timm\n","from timm.data import resolve_data_config\n","from timm.data.transforms_factory import create_transform\n","\n","model = timm.create_model('vit_base_patch16_224', pretrained=True)"]},{"cell_type":"markdown","source":["# Upload dataset"],"metadata":{"id":"NxaWIYEvV7Uk"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yaJWHEPhBj6Z","outputId":"64bef0ed-cacd-484a-8b03-52d51e742d60","executionInfo":{"status":"ok","timestamp":1718959859261,"user_tz":-540,"elapsed":19194,"user":{"displayName":"22 ML","userId":"00831946089676507549"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjDJtr0qK94z"},"outputs":[],"source":["from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader, Subset\n","from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n","import numpy as np\n","\n","transform = Compose([\n","    Resize((224, 224)),\n","    ToTensor(),\n","    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","dataset = ImageFolder('/content/drive/MyDrive/Project/train', transform=transform)\n","\n","indices = np.random.choice(len(dataset), 3000, replace=False)\n","subset_dataset = Subset(dataset, indices)\n","\n","batch_size = 32\n","data_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCXYj1XjG6Qs"},"outputs":[],"source":["val_transform = Compose([\n","    Resize((224, 224)),\n","    ToTensor(),\n","    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","val_dataset = ImageFolder('/content/drive/My Drive/Project/valid', transform=val_transform)\n","\n","val_indices = np.random.choice(len(val_dataset), 300, replace=False)\n","subset_val_dataset = Subset(val_dataset, val_indices)\n","\n","val_loader = DataLoader(subset_val_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","source":["# PEFT LoRA 적용"],"metadata":{"id":"XLwgHlADmslV"}},{"cell_type":"code","source":["pip install peft"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eq61p_NOcSPP","executionInfo":{"status":"ok","timestamp":1718960142022,"user_tz":-540,"elapsed":7351,"user":{"displayName":"22 ML","userId":"00831946089676507549"}},"outputId":"a5e992cd-994b-48a5-ba14-138daf1b58d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting peft\n","  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/251.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m204.8/251.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n","Collecting accelerate>=0.21.0 (from peft)\n","  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.40)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Installing collected packages: accelerate, peft\n","Successfully installed accelerate-0.31.0 peft-0.11.1\n"]}]},{"cell_type":"code","source":["def print_trainable_parameters(model):\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n","    )\n","\n","print_trainable_parameters(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VAnCU9qjg6BL","executionInfo":{"status":"ok","timestamp":1718960142023,"user_tz":-540,"elapsed":6,"user":{"displayName":"22 ML","userId":"00831946089676507549"}},"outputId":"ca27679c-48c9-42e8-d4c6-f42f1ab5a3ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 86567656 || all params: 86567656 || trainable%: 100.00\n"]}]},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model\n","\n","config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules=[\"attn.qkv\"],\n","    modules_to_save=[\"classifier\"],\n",")\n","\n","lora_model = get_peft_model(model, config)"],"metadata":{"id":"5CD2hEHihSWK","executionInfo":{"status":"ok","timestamp":1718960145027,"user_tz":-540,"elapsed":3007,"user":{"displayName":"22 ML","userId":"00831946089676507549"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ac9960b-121e-4fe4-9be8-e6b00e4413d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"]}]},{"cell_type":"code","source":["print_trainable_parameters(lora_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4rAM_eCmoND","executionInfo":{"status":"ok","timestamp":1718715213638,"user_tz":-540,"elapsed":509,"user":{"displayName":"5기김정현","userId":"05713384309589019705"}},"outputId":"a9c42d5a-3481-4945-b847-a790b90a5c53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 589824 || all params: 87157480 || trainable%: 0.68\n"]}]},{"cell_type":"markdown","source":["# Naive Model"],"metadata":{"id":"RhtJDwQSYACB"}},{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"dhhkUvurmwWt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tvx1sH8tFN1k"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","import torch\n","import os\n","\n","num_classes = 2\n","model.head = nn.Linear(model.head.in_features, num_classes)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","source":["\n","def validate(model, data_loader, criterion, device):\n","    model.eval()\n","    model.to(device)\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(data_loader)\n","    accuracy = 100 * correct / total\n","    return avg_loss, accuracy\n","\n","model_dir = '/content/drive/MyDrive/Project/Vit_model_save'\n","os.makedirs(model_dir, exist_ok=True)"],"metadata":{"id":"hLcT8Zs3yyG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuC8KLqpIhRb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718954293375,"user_tz":-540,"elapsed":858627,"user":{"displayName":"22 ML","userId":"00831946089676507549"}},"outputId":"9fbd0d89-bde7-4de6-a500-214096060830"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Training Loss: 0.04712603613734245\n","Epoch 1, Validation Loss: 0.07327490374445915, Accuracy: 97.0%\n","Epoch 2, Training Loss: 0.08074439316987991\n","Epoch 2, Validation Loss: 0.23631146773695946, Accuracy: 91.66666666666667%\n","Epoch 3, Training Loss: 0.0009470694349147379\n","Epoch 3, Validation Loss: 0.9849343985319138, Accuracy: 81.0%\n","Epoch 4, Training Loss: 0.00014174157695379108\n","Epoch 4, Validation Loss: 0.16321686680894346, Accuracy: 95.0%\n","Epoch 5, Training Loss: 0.001646352931857109\n","Epoch 5, Validation Loss: 0.1349247632548213, Accuracy: 95.0%\n","Model saved at: /content/drive/MyDrive/Project/Vit_model_save/Naive model_epoch_5.pth\n","Epoch 6, Training Loss: 0.0006583644426427782\n","Epoch 6, Validation Loss: 0.14288764335215093, Accuracy: 95.66666666666667%\n","Epoch 7, Training Loss: 0.0001694320817478001\n","Epoch 7, Validation Loss: 0.3097483916208148, Accuracy: 90.33333333333333%\n","Epoch 8, Training Loss: 0.14643724262714386\n","Epoch 8, Validation Loss: 0.14175594663247465, Accuracy: 94.66666666666667%\n","Epoch 9, Training Loss: 0.0030910177156329155\n","Epoch 9, Validation Loss: 0.3927910603582859, Accuracy: 88.66666666666667%\n","Epoch 10, Training Loss: 0.003617635229602456\n","Epoch 10, Validation Loss: 0.21234280057251453, Accuracy: 94.66666666666667%\n","Model saved at: /content/drive/MyDrive/Project/Vit_model_save/Naive model_epoch_10.pth\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    for images, labels in data_loader:\n","        c\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch {epoch+1}, Training Loss: {loss.item()}')\n","\n","    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n","    print(f'Epoch {epoch+1}, Validation Loss: {val_loss}, Accuracy: {val_accuracy}%')\n","\n","    if (epoch + 1) % 5 == 0:\n","        model_name = f'Naive model_epoch_{epoch+1}.pth'\n","        model_path = os.path.join(model_dir, model_name)\n","        torch.save(model.state_dict(), model_path)\n","        print(f'Model saved at: {model_path}')"]},{"cell_type":"markdown","source":["## Import saved model"],"metadata":{"id":"iRgPAhwjWQiI"}},{"cell_type":"code","source":["base_model = timm.create_model('vit_base_patch16_224', pretrained=False)\n","base_model.head = nn.Linear(base_model.head.in_features, num_classes)\n","\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules=[\"attn.qkv\"],\n","    modules_to_save=[\"classifier\"],\n",")\n","\n","loaded_model = get_peft_model(base_model, lora_config)\n","\n","model_path = '/content/drive/MyDrive/Project/Vit_model_save/model_epoch_9.pth'\n","loaded_model.load_state_dict(torch.load(model_path), strict=False)\n","loaded_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plGc3EAJaokw","executionInfo":{"status":"ok","timestamp":1718684872514,"user_tz":-540,"elapsed":2291,"user":{"displayName":"­정재희 | 데이터사이언스전공 | 한양대(서울)","userId":"06340913878035256272"}},"outputId":"bea5b7e1-1cf1-4f25-c3fb-d6fc85ada793"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PeftModel(\n","  (base_model): LoraModel(\n","    (model): VisionTransformer(\n","      (patch_embed): PatchEmbed(\n","        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","        (norm): Identity()\n","      )\n","      (pos_drop): Dropout(p=0.0, inplace=False)\n","      (patch_drop): Identity()\n","      (norm_pre): Identity()\n","      (blocks): Sequential(\n","        (0): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (1): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (2): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (3): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (4): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (5): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (6): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (7): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (8): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (9): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (10): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","        (11): Block(\n","          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (attn): Attention(\n","            (qkv): lora.Linear(\n","              (base_layer): Linear(in_features=768, out_features=2304, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=16, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=16, out_features=2304, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (q_norm): Identity()\n","            (k_norm): Identity()\n","            (attn_drop): Dropout(p=0.0, inplace=False)\n","            (proj): Linear(in_features=768, out_features=768, bias=True)\n","            (proj_drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls1): Identity()\n","          (drop_path1): Identity()\n","          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (drop1): Dropout(p=0.0, inplace=False)\n","            (norm): Identity()\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop2): Dropout(p=0.0, inplace=False)\n","          )\n","          (ls2): Identity()\n","          (drop_path2): Identity()\n","        )\n","      )\n","      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (fc_norm): Identity()\n","      (head_drop): Dropout(p=0.0, inplace=False)\n","      (head): Linear(in_features=768, out_features=2, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"_cctDD3fRyEP"}},{"cell_type":"code","source":["import os\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision.io import read_image\n","from torchvision.transforms.functional import to_pil_image\n","from torchvision import transforms, datasets\n","from tqdm import tqdm\n","import pandas as pd\n","\n","testset_path = '/content/drive/MyDrive/Project/test'\n","\n","transform = Compose([\n","    Resize((224, 224)),\n","    ToTensor(),\n","    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","testset = datasets.ImageFolder(root=testset_path, transform=transform)\n","test_loader = DataLoader(testset, batch_size=1, shuffle=False)"],"metadata":{"id":"BEN7TtFvRx1d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Only LoRA with CrossEntropy"],"metadata":{"id":"JdIPSnJuU5kG"}},{"cell_type":"code","source":["if len(testset) == 500:\n","    results = []\n","    model.eval()\n","    model.to(device)\n","    for images, _ in tqdm(test_loader, total=len(test_loader)):\n","        images = images.to(device)\n","        outputs = model(images)\n","\n","        if isinstance(outputs, dict) and 'logits' in outputs:\n","            logits = outputs['logits'].cpu()\n","        else:\n","            logits = outputs.cpu()\n","\n","        _, predicted = torch.max(logits, dim=1)\n","        label = 'real' if predicted.item() == 1 else 'generated'\n","        results.append([label])\n","\n","    df = pd.DataFrame(results, columns=['label'])\n","    results_path = '/content/drive/MyDrive/Project/test/results_naive.csv'\n","    df.to_csv(results_path, index=False)\n","    print(f'Result stored: {results_path}')\n","\n","else:\n","    print(\"Number of datasets is incorrect. Number of verified data:\", len(testset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MVJQbK6UU4Or","executionInfo":{"status":"ok","timestamp":1718956018532,"user_tz":-540,"elapsed":162514,"user":{"displayName":"22 ML","userId":"00831946089676507549"}},"outputId":"1257b728-16e6-4700-d968-277527ab77d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [02:42<00:00,  3.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Result stored: /content/drive/MyDrive/Project/test/results_naive.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["def calcul_accuracy(predicted_labels, labels_csv_path):\n","    true_labels_df = pd.read_csv(labels_csv_path, sep=';')\n","    true_labels = true_labels_df['label'].values\n","    predicted_labels_flat = [label[0] for label in predicted_labels]\n","\n","    matches = true_labels == predicted_labels_flat\n","    correct_predictions = matches.sum()\n","\n","    print(f\"Correct predictions: {correct_predictions} out of {len(matches)}\")\n","\n","    accuracy = correct_predictions / len(matches)\n","\n","    return accuracy"],"metadata":{"id":"bqzijpyMfoOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_labels = results\n","labels_csv_path = '/content/drive/MyDrive/Project/test/test_labels.csv'\n","accuracy = calcul_accuracy(predicted_labels, labels_csv_path)\n","print(f\"Accuracy: {accuracy*100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJe23Aw7WXqF","executionInfo":{"status":"ok","timestamp":1718956235799,"user_tz":-540,"elapsed":432,"user":{"displayName":"22 ML","userId":"00831946089676507549"}},"outputId":"545b3cb6-f264-400d-fdf3-0025ebee9c8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct predictions: 420 out of 500\n","Accuracy: 84.00%\n"]}]},{"cell_type":"markdown","source":["## Valid with generated image"],"metadata":{"id":"r_mx4OqWtX1L"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","import numpy as np\n","import os\n","from PIL import Image\n","\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","extra_val_path = '/content/drive/MyDrive/Project/GeneratedImages/generated'\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, image_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.lower().endswith(('jpg'))]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","        label = 0\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","extra_val_dataset = CustomImageDataset(image_dir=extra_val_path, transform=val_transform)\n","extra_val_indices = np.random.choice(len(extra_val_dataset), 100, replace=False)\n","extra_subset_val_dataset = Subset(extra_val_dataset, extra_val_indices)\n","extra_val_loader = DataLoader(extra_subset_val_dataset, batch_size=32, shuffle=False)\n","\n","def validate_1(model, val_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            outputs = model(images)\n","            logits = outputs['logits']\n","            loss = criterion(logits, labels)\n","\n","            total_loss += loss.item()\n","            _, predicted = torch.max(logits, dim=1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(val_loader)\n","    accuracy = 100 * correct / total\n","    print(f'Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy}%')\n","    return avg_loss, accuracy\n","\n","model = ModelWrapper(loaded_model)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","\n","new_validation_loss, new_validation_accuracy = validate_1(model, extra_val_loader, criterion, device)\n","\n","print(f'(Generated) Validation Loss: {new_validation_loss:.4f}')\n","print(f'(Generated) Validation Accuracy: {new_validation_accuracy:.2f}%')"],"metadata":{"id":"XC532Jxt1Y1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SCL Model"],"metadata":{"id":"pJx-sVkFYEIq"}},{"cell_type":"markdown","source":["## SCL loss func"],"metadata":{"id":"f6s5rqkndwoy"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import os\n","\n","def single_center_loss(features, labels, margin=1.0):\n","    real_mask = labels == 1\n","    generated_mask = labels == 0\n","\n","    real_features = features[real_mask]\n","    generated_features = features[generated_mask]\n","\n","    if real_features.size(0) == 0 or generated_features.size(0) == 0:\n","        return torch.tensor(0.0, device=features.device)\n","\n","    center = real_features.mean(dim=0)\n","    dreal = (real_features - center).norm(dim=1).mean()\n","    dgenerated = (generated_features - center).norm(dim=1).mean()\n","\n","    return dreal + torch.clamp(dreal - dgenerated + margin, min=0.0)\n","\n","class ModelWrapper(nn.Module):\n","    def __init__(self, peft_model):\n","        super().__init__()\n","        self.peft_model = peft_model.base_model.model\n","\n","    def forward(self, x):\n","        features = self.peft_model.patch_embed(x)\n","        features = self.peft_model.pos_drop(features)\n","\n","        for block in self.peft_model.blocks:\n","            features = block(features)\n","\n","        gap_features = torch.mean(features, dim=1)\n","        logits = self.peft_model.head(gap_features)\n","\n","        return {'logits': logits, 'features': gap_features}\n","\n","def train_SCL(model, train_loader, criterion, optimizer, device, scl_weight=0.1):\n","    model.train()\n","    total_loss = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        logits = outputs['logits']\n","        features = outputs['features']\n","\n","        ce_loss = criterion(logits, labels)\n","        scl_loss = single_center_loss(features, labels)\n","        loss = ce_loss + scl_weight * scl_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f'Training Loss: {avg_loss}')\n","    return avg_loss\n","\n","def validate_SCL(model, val_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            logits = outputs['logits']\n","            loss = criterion(logits, labels)\n","\n","            total_loss += loss.item()\n","            _, predicted = torch.max(logits, dim=1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(val_loader)\n","    accuracy = 100 * correct / total\n","    print(f'Validation Loss: {avg_loss}, Accuracy: {accuracy}%')\n","    return avg_loss, accuracy"],"metadata":{"id":"WDuv6fT867k7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = ModelWrapper(lora_model)\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","train_loader = data_loader\n","valid_loader = val_loader\n","model_dir = '/content/drive/MyDrive/ArtificialIntelligence/Project/Vit_model_save'\n","os.makedirs(model_dir, exist_ok=True)\n","\n","epochs = 10\n","scl_weight = 0.1\n","for epoch in range(epochs):\n","    train_loss = train_SCL(model, train_loader, criterion, optimizer, device, scl_weight)\n","    val_loss, val_accuracy = validate_SCL(model, valid_loader, criterion, device)\n","\n","    if (epoch + 1) % 5 == 0:\n","        model_path = os.path.join(model_dir, f'(refined SCL) model_epoch_{epoch+1}.pth')\n","        torch.save(model.state_dict(), model_path)\n","        print(f'Model saved at: {model_path}')"],"metadata":{"id":"zp03zhu57BX-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718698657943,"user_tz":-540,"elapsed":2520321,"user":{"displayName":"안예빈","userId":"12603601147266511798"}},"outputId":"27c917dc-0e45-4b4f-97b0-0356b886d351"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Loss: 0.8302590346082728\n","Validation Loss: 1.4234132051467896, Accuracy: 4.666666666666667%\n","Training Loss: 0.7945339330967437\n","Validation Loss: 1.0747916877269745, Accuracy: 4.666666666666667%\n","Training Loss: 0.7968879678147904\n","Validation Loss: 0.8638421714305877, Accuracy: 4.666666666666667%\n","Training Loss: 0.7736934315651021\n","Validation Loss: 1.055364626646042, Accuracy: 4.666666666666667%\n","Training Loss: 0.7715306193270581\n","Validation Loss: 0.9156368315219879, Accuracy: 4.666666666666667%\n","Model saved at: /content/drive/MyDrive/ArtificialIntelligence/Project/Vit_model_save/(refined SCL) model_epoch_5.pth\n","Training Loss: 0.7744804233946698\n","Validation Loss: 0.8319910228252411, Accuracy: 4.666666666666667%\n","Training Loss: 0.7947372775128547\n","Validation Loss: 1.5643630027770996, Accuracy: 4.666666666666667%\n","Training Loss: 0.7738565510891854\n","Validation Loss: 1.2858258008956909, Accuracy: 4.666666666666667%\n","Training Loss: 0.7048082795548947\n","Validation Loss: 1.259290611743927, Accuracy: 16.666666666666668%\n","Training Loss: 0.6396270540166409\n","Validation Loss: 0.8148479998111725, Accuracy: 41.333333333333336%\n","Model saved at: /content/drive/MyDrive/ArtificialIntelligence/Project/Vit_model_save/(refined SCL) model_epoch_10.pth\n"]}]},{"cell_type":"markdown","source":["## Test"],"metadata":{"id":"dDzOaxnTYOMy"}},{"cell_type":"code","source":["import os\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision.io import read_image\n","from torchvision.transforms.functional import to_pil_image\n","from torchvision import transforms, datasets\n","from tqdm import tqdm\n","import pandas as pd\n","\n","testset_path = '/content/drive/MyDrive/Project/test'\n","\n","transform = Compose([\n","    Resize((224, 224)),\n","    ToTensor(),\n","    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","testset = datasets.ImageFolder(root=testset_path, transform=transform)\n","test_loader = DataLoader(testset, batch_size=1, shuffle=False)"],"metadata":{"id":"FeHWW5KjYOMy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"-8FIF--DYOMz"}},{"cell_type":"code","source":["if len(testset) == 500:\n","    print(\"The number of testset: 500\")\n","\n","    results = []\n","    model.eval()\n","    for images, _ in tqdm(test_loader, total=len(test_loader)):\n","        outputs = model(images)\n","\n","        if 'logits' in outputs:\n","            logits = outputs['logits']\n","            _, predicted = torch.max(logits, 1)\n","            label = 'real' if predicted.item() == 1 else 'generated'\n","            results.append([label])\n","        else:\n","            print(\"Logits not found in model output.\")\n","\n","    df = pd.DataFrame(results, columns=['label'])\n","    results_path = '/content/drive/MyDrive/Project/ArtificialIntelligence/test/results_scl.csv'\n","    df.to_csv(results_path, index=False)\n","    print(f'Result stored: {results_path}')\n","else:\n","    print(\"Number of datasets is incorrect. Verified data count:\", len(testset))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"error","timestamp":1718699091999,"user_tz":-540,"elapsed":392,"user":{"displayName":"­정재희 | 데이터사이언스전공 | 한양대(서울)","userId":"06340913878035256272"}},"outputId":"c1a72032-d55f-4405-e6fc-60f0543ad990","id":"o1X7W_CeYOMz"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'testset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6e827aed3882>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The number of testset: 500\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'testset' is not defined"]}]},{"cell_type":"code","source":["def calcul_accuracy(predicted_labels, labels_csv_path):\n","    true_labels_df = pd.read_csv(labels_csv_path, sep=';')\n","    true_labels = true_labels_df['label'].values\n","    predicted_labels_flat = [label[0] for label in predicted_labels]\n","\n","    matches = true_labels == predicted_labels_flat\n","    correct_predictions = matches.sum()\n","\n","    print(f\"Correct predictions: {correct_predictions} out of {len(matches)}\")\n","\n","    accuracy = correct_predictions / len(matches)\n","\n","    return accuracy"],"metadata":{"id":"BHBV6RI2YOMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_labels = results\n","labels_csv_path = '/content/drive/MyDrive/Project/test/test_labels.csv'\n","accuracy = calcul_accuracy(predicted_labels, labels_csv_path)\n","print(f\"Accuracy: {accuracy*100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"error","timestamp":1718700350549,"user_tz":-540,"elapsed":21,"user":{"displayName":"­정재희 | 데이터사이언스전공 | 한양대(서울)","userId":"06340913878035256272"}},"outputId":"a2de9708-f3ba-4cd8-f3cf-29f4ab94d702","id":"LboXRXrCYOM0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'calcul_accuracy' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-df77ae2a743d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels_csv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Project/test/test_labels.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcul_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_csv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy: {accuracy*100:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'calcul_accuracy' is not defined"]}]},{"cell_type":"markdown","source":["##load model"],"metadata":{"id":"D4pGUpTtCDEm"}},{"cell_type":"code","source":["base_model = timm.create_model('vit_base_patch16_224', pretrained=False)\n","base_model.head = nn.Linear(base_model.head.in_features, num_classes)\n","\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    target_modules=[\"attn.qkv\"],\n","    modules_to_save=[\"classifier\"],\n",")\n","\n","loaded_model = get_peft_model(base_model, lora_config)\n","\n","model_path = '/content/drive/MyDrive/Project/Vit_model_save/Naive model_epoch_10.pth'\n","loaded_model.load_state_dict(torch.load(model_path), strict=False)\n","loaded_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"bI2nd3aeO9cv","executionInfo":{"status":"error","timestamp":1718959459943,"user_tz":-540,"elapsed":10,"user":{"displayName":"22 ML","userId":"00831946089676507549"}},"outputId":"45e1376d-fdb5-4aae-9a2f-5fcd4e52056a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'timm' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-883966217311>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vit_base_patch16_224'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m lora_config = LoraConfig(\n\u001b[1;32m      5\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'timm' is not defined"]}]},{"cell_type":"code","source":["model = ModelWrapper(loaded_scl_model)\n","\n","if len(testset) == 500:\n","    print(\"The number of testset: 500\")\n","\n","    results = []\n","    model.eval()\n","    for images, _ in tqdm(test_loader, total=len(test_loader)):\n","        outputs = model(images)\n","\n","        if 'logits' in outputs:\n","            logits = outputs['logits']\n","            _, predicted = torch.max(logits, 1)\n","            label = 'real' if predicted.item() == 1 else 'generated'\n","            results.append([label])\n","        else:\n","            print(\"Logits not found in model output.\")\n","\n","    df = pd.DataFrame(results, columns=['label'])\n","    results_path = '/content/drive/MyDrive/Project/test/results_scl_2.csv'\n","    df.to_csv(results_path, index=False)\n","    print(f'Result stored: {results_path}')\n","else:\n","    print(\"Number of datasets is incorrect. Verified data count:\", len(testset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zEFxKi5akYmn","executionInfo":{"status":"ok","timestamp":1718700350549,"user_tz":-540,"elapsed":347497,"user":{"displayName":"­정재희 | 데이터사이언스전공 | 한양대(서울)","userId":"06340913878035256272"}},"outputId":"71fbfde3-4854-4aba-b90f-61a7f8a16a4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of testset: 500\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 500/500 [05:47<00:00,  1.44it/s]"]},{"output_type":"stream","name":"stdout","text":["Result stored: /content/drive/MyDrive/Project/test/results_scl_2.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["predicted_labels = results\n","labels_csv_path = '/content/drive/MyDrive/Project/test/test_labels.csv'\n","accuracy = calcul_accuracy(predicted_labels, labels_csv_path)\n","print(f\"(SCL) Accuracy: {accuracy*100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvF0B8ajczW9","executionInfo":{"status":"ok","timestamp":1718700376472,"user_tz":-540,"elapsed":835,"user":{"displayName":"­정재희 | 데이터사이언스전공 | 한양대(서울)","userId":"06340913878035256272"}},"outputId":"29895c53-4f93-48af-91e5-d3ec40f8c144"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct predictions: 421 out of 500\n","(SCL) Accuracy: 84.20%\n"]}]},{"cell_type":"markdown","source":["## Valid with generated image"],"metadata":{"id":"abZPB56QYOM1"}},{"cell_type":"code","source":["def validate(model, data_loader, criterion, device):\n","    model.eval()\n","    model.to(device)\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(data_loader)\n","    accuracy = 100 * correct / total\n","    return avg_loss, accuracy"],"metadata":{"id":"97BBr_I2zyvo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = loaded_model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","new_validation_loss, new_validation_accuracy = validate(model, extra_val_loader, criterion, device)\n","\n","print(f'(Generated) Validation Loss: {new_validation_loss:.4f}')\n","print(f'(Generated) Validation Accuracy: {new_validation_accuracy:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"id":"kiPP9EkaxhHm","executionInfo":{"status":"error","timestamp":1718700826245,"user_tz":-540,"elapsed":371,"user":{"displayName":"­정재희 | 데이터사이언스전공 | 한양대(서울)","userId":"06340913878035256272"}},"outputId":"106ef06e-5958-411c-e768-2015d5cc938b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'loaded_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-7fa8b1f9b942>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_validation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_validation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'loaded_model' is not defined"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import datasets, transforms\n","import numpy as np\n","import os\n","\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","extra_val_path = '/content/drive/MyDrive/Project/GeneratedImages'\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","extra_val_dataset = datasets.ImageFolder(root=extra_val_path, transform=val_transform)\n","extra_val_indices = np.random.choice(len(extra_val_dataset), 100, replace=False)\n","extra_subset_val_dataset = Subset(extra_val_dataset, extra_val_indices)\n","\n","class OneLabelDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, label):\n","        self.dataset = dataset\n","        self.label = label\n","\n","    def __getitem__(self, index):\n","        image, _ = self.dataset[index]\n","        return image, torch.tensor(self.label, dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","extra_subset_val_dataset = OneLabelDataset(extra_subset_val_dataset, 0)\n","\n","extra_val_loader = DataLoader(extra_subset_val_dataset, batch_size=32, shuffle=False)\n","\n","def validate_SCL(model, val_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            logits = outputs['logits']\n","            loss = criterion(logits, labels)\n","\n","            total_loss += loss.item()\n","            _, predicted = torch.max(logits, dim=1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_loss = total_loss / len(val_loader)\n","    accuracy = 100 * correct / total\n","    print(f'Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy}%')\n","    return avg_loss, accuracy\n","\n","model = loaded_scl_model\n","model = ModelWrapper(model)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","\n","new_validation_loss, new_validation_accuracy = validate_SCL(model, extra_val_loader, criterion, device)\n","\n","print(f'(Generated / SCL) Validation Loss: {new_validation_loss:.4f}')\n","print(f'(Generated / SCL) Validation Accuracy: {new_validation_accuracy:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dyYRryFll2d","executionInfo":{"status":"ok","timestamp":1718703447786,"user_tz":-540,"elapsed":3673,"user":{"displayName":"Jaehee Jung","userId":"14887456151047057246"}},"outputId":"72930f2a-2e32-4067-d065-b2a8e404c218"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 0.6208, Accuracy: 88.0%\n","(Generated / SCL) Validation Loss: 0.6208\n","(Generated / SCL) Validation Accuracy: 88.00%\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","collapsed_sections":["adLDPo9KVxLG","NxaWIYEvV7Uk","XLwgHlADmslV","dhhkUvurmwWt","iRgPAhwjWQiI","_cctDD3fRyEP","JdIPSnJuU5kG","f6s5rqkndwoy","r_mx4OqWtX1L"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4f8a484727bf41d88f619c4fd2b9a24e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02149e2905b041af8e110243186e6a38","IPY_MODEL_98e0b5be2ac24819a12ee22e3264c3ef","IPY_MODEL_6e2fa310759342bebd04d82325cd248a"],"layout":"IPY_MODEL_06b56f7d430f44fcb1057e9f02ffa588"}},"02149e2905b041af8e110243186e6a38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a5393c976c04740b45966874927933b","placeholder":"​","style":"IPY_MODEL_4d021402675e41cfa8edc31b7cec3c5e","value":"model.safetensors: 100%"}},"98e0b5be2ac24819a12ee22e3264c3ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02e092874ae349faa22a44593e3c9654","max":346284714,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af67225a89bc4479a4bb9794bfa6c1cc","value":346284714}},"6e2fa310759342bebd04d82325cd248a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c449fe02d6c64ecb8f08be0e0bffec37","placeholder":"​","style":"IPY_MODEL_a200054d3d81493da8a6542b4ff50ea8","value":" 346M/346M [00:04&lt;00:00, 92.7MB/s]"}},"06b56f7d430f44fcb1057e9f02ffa588":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a5393c976c04740b45966874927933b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d021402675e41cfa8edc31b7cec3c5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02e092874ae349faa22a44593e3c9654":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af67225a89bc4479a4bb9794bfa6c1cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c449fe02d6c64ecb8f08be0e0bffec37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a200054d3d81493da8a6542b4ff50ea8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}